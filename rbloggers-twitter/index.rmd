---
title: "Analyzing R-Bloggers' posts via Twitter"
output:
  html_document:
    toc: true
author: Dean Attali
date: "2015-05-15"
runtime: shiny
---

(lots of cutting corners: not considering any other social media, not looking at # of times post was shared directly from r-bloggers website, not looking at how much discussion the post generated by only at number of favorites and retweets)
didnt do statistical significance tests, didnt include fb data, didnt include # of shares via tweet button because it looks like most posts pre 2014 mostly have 0 so it'll introduce an unfair bias towards new posts

## Results

```{r, fig.width=10, echo = FALSE}
library(DT)
library(ggvis)
library(plyr)
library(dplyr)

if (!file.exists(file.path("results", "tweets.rds"))) {
print("Could not find tweets file.")
} else {
tweets <- readRDS("tweets.rds")
  
tweets_by_author <-
  tweets %>% 
  ddply(~ author, function(x) {
    data.frame(num_tweets = nrow(x),
               avg_favorites = mean(x$favorites) %>% round(digits = 1),
               avg_retweets = mean(x$retweets) %>% round(digits = 1),
               avg_score = mean(x$score) %>% round(digits = 1)
    )}) %>%
    arrange(desc(avg_score)) 

top_authors <- tweets_by_author %>% arrange(desc(avg_score)) %>% .$author %>% head(10)
tweets_top_authors <- tweets %>% filter(author %in% top_authors)

datatable(
  tweets_by_author,
  rownames = FALSE,
  escape = TRUE,
  class = 'cell-border stripe hover',
  options = list(searching = FALSE),
  caption = htmltools::tags$caption(
    style = 'caption-side: bottom; text-align: center; color: #777777;',
    "Searching and filtering has been disabled since this is hosted on a weak machine and I don't want to crash to poor guy"
  )
)

datatable(
  tweets %>% select(date, title, author, retweets, favorites, score, url),
  rownames = FALSE,
  escape = TRUE,
  class = 'cell-border stripe hover',
  caption = htmltools::tags$caption(
    style = 'caption-side: bottom; text-align: center; color: #777777;',
    "Searching and filtering has been disabled since this is hosted on a weak machine and I don't want to crash to poor guy"
  ),
  options = list(
    searching = FALSE,
    pageLength = 25,
    columnDefs = list(
      # replace - in dates to . so that the string won't get cut into multiple lines
      list(targets = 0,
           render = JS(
             "function(data, type, row, meta) {",
               "return data.replace(/-/g, '.');",
             "}"
      )),
      # make the title link to the blog pots
      list(targets = 6,
           render = JS(
             "function(data, type, row, meta) {",
               "return '<a target=\"_blank\" href=\"' + data + '\">' + data + '</a>';",
             "}"
      ))
    )
  )
)

tooltip_value <- function(x) {
  if(is.null(x)) return(NULL)
  
  idx <- which(tweets_top_authors$favorites == x$favorites &
               tweets_top_authors$retweets == x$retweets &
               tweets_top_authors$author == x$author)
  tweet <- tweets_top_authors[idx, ]
  res <-
    paste0("<strong>", tweet$title, "</strong><br/>",
           tweet$date, "<br/>",
           paste0(names(x), ": ", x, collapse = "<br />"))
  res
}

tweets_top_authors %>%
  ggvis(x = ~ favorites, y = ~ retweets, fill = ~ author, stroke := "black") %>%
  layer_points(size := 100) %>%
  add_tooltip(tooltip_value) %>%
  add_axis("x",
           title = "# favorites",
           properties = axis_props(
             title = list(fontSize = 25),
             labels = list(fontSize = 25)
           ),
           title_offset = 50
  ) %>%
  add_axis("y",
           title = "# retweets" ,
           properties = axis_props(
             title = list(fontSize = 25),
             labels = list(fontSize = 25)    
           ),
           title_offset = 50
  ) %>%
  add_legend("fill",
             title = "Author",
             properties = legend_props(
               title = list(fontSize = 22),
               symbols = list(size = 150),
               labels = list(fontSize = 20))
             )
}
```


## Code

```{r knitr-setup}
knitr::opts_chunk$set(eval = FALSE, message=FALSE, warning=FALSE)
```

```{r load-pkgs}
library(httr)
library(XML)
library(plyr)
library(dplyr)
library(magrittr)
library(twitteR)
library(ggplot2)
library(ggvis)
library(DT)

# for wordcloud
library(SnowballC)
library(wordcloud)
library(tm)
library(stringr)
```

Setup twitter oauth.  
These private keys are stored in my .Rprofile. To get this to work, get your own tokens from Twitter and set them in your .Rprofile
using `options(twitter_consumer_key = "YOUR_CONSUMER_KEY")`.

```{r setup-twitter}
setup_twitter_oauth(getOption('twitter_consumer_key'),
                    getOption('twitter_consumer_secret'),
                    getOption('twitter_access_token'),
                    getOption('twitter_access_secret'))
```

Grab the last 3200 tweets (restriction set by Twitter) and prepare a nice data.frame with the info we want.  
I'm keeping:  

- tweet ID  

- tweet date  

- day of the week that the tweet was made  

- \# of times tweet was favorited  

- \# of times tweet was retweeted  

- tweet text (= blog post title)  

- last URL in each tweet, becuse that's the URL that points to the r-bloggers post

```{r get-tweets}
MAX_TWEETS <- 3200
tweets_raw <- userTimeline('Rbloggers', n = MAX_TWEETS,
                           includeRts = FALSE, excludeReplies = TRUE)

tweets <- 
  ldply(tweets_raw, function(x) {
    data_frame(id = x$id,
               date = as.Date(x$created),
               day = weekdays(date),
               favorites = x$favoriteCount,
               retweets = x$retweetCount,
               title = x$text,
               url = x$urls %>% .[['url']] %>% tail(1)
    )
  })

rm(tweets_raw)  # being extremely memory conscious
```

Add author name to each post.

This is achieved by following the URL of each tweet and scraping the resulting R-bloggers post to find the author tag. If the author did not provide a name and is just an email address, R-Bloggers tried to hide their email address and we have to jump through a tiny hoop to grab it.

```{r add-authors}
# Get the author of a single post given an R-Bloggers post URL
get_post_author_single <- function(url) {
  if (is.null(url)) {
    return(NA)
  }

  # get author HTML node
  author_node <- 
    GET(url) %>%
    httr::content("parsed") %>%
    getNodeSet("//a[@rel='author']")
  if (author_node %>% length != 1) {
    return(NA)
  }

  # get author name
  author <- author_node %>% .[[1]] %>% xmlValue

  # r-bloggers hides email address names so grab the email a different way
  if (nchar(author) > 100 && grepl("document\\.getElementsByTagName", author)) {
    author <- author_node %>% .[[1]] %>% xmlGetAttr("title")
  }

  author  
}

# Get a list of URL --> author for a list of R-Bloggers post URLs
get_post_author <- function(urls) {
  lapply(urls, get_post_author_single) %>% unlist
}

# Add the author to each tweet.
# This will take several minutes because we're scraping r-bloggers 3200 times (don't run this frequently - we don't want to overwork our beloved R-Bloggers server)
tweets %<>% mutate(author = get_post_author(url))  

# Remove NA author (these are mostly jobs postings, plus a few blog posts that have been deleted)
tweets %<>% na.omit
```

Calculate a score metric for every tweet using the little bit of information we have (# of favorites and # of retweets). This is of course very arbitrary - I chose to score a tweet's success as a linear combination of its # of favorites and retweets.  Since there are roughly twice as many favorites as retweets in total, retweets get twice the weight. Very simple formula :)

```{r add-score}
sum(tweets$favorites) / sum(tweets$retweets)   # = 2.1
tweets$score <- tweets$favorites + tweets$retweets * 2
```

Now we have all the necessary information in the `tweets` data.frame, so it's time for a bit of cleanup:  

- Remove the URL and `#rstats` hashtag from every tweet's title  

- Older posts all contain the text "This article was originally posted on ... and kindly contributed by ...". Try to remove that as well.

- Order the day factor levels in order from Monday - Sunday

- Truncate very long author names with an ellipsis

- Merge duplicate tweets (tweets with the same author and title that are posted within a week)

```{r cleanup-tweets}
# remove redundant URL+hashtag from title
tweets$title <- mapply(gsub, sprintf(" %s #rstats", tweets$url), "", tweets$title)

# remove redundant "This article was originally published..." from title
tweets$title <- gsub(": \n\n\\(This article.*$", "", tweets$title)

# order days of the week
tweets$day %<>% factor(levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))

# truncate long author names
ellipsis <- function(x, n) {
  idx <- nchar(x) > n
  x[idx] <- paste0(substr(x[idx], 1, n-3), "...")
  x
}
tweets$author %<>% ellipsis(35)

# Merge duplicate tweets.
# There are some tweets with the exact same title and author that are a day apart for some reason, and they have different URLs because they do appear on r-bloggers twice, so here I'm trying to merge these cases. It's not the most efficient way to do it, but it's readable so I'll stick with it.
days_btwn_posts <- function(x) {
  (x$date %>% head(1) - x$date %>% tail(1)) %>% as.integer
}
tweets %<>%
  ddply(.(author, title), function(x) {
    if (nrow(x) > 1 && days_btwn_posts(x) <= 7) {
      x$favorites <- sum(x$favorites)
      x$retweets <- sum(x$retweets)
      x <- x[1, , drop = FALSE]
    }
    x
  }) %>%
  arrange(desc(date))
```

Save the results so that we can load them up later without doing the slow API calls + scraping again. Save it both as a csv so that humans can easily take a peek and as rds to load it into R faster and more accurately.

```{r save-tweets}
resultsDir <- "results"
dir.create(resultsDir, showWarnings = FALSE)
write.csv(tweets, file.path(resultsDir, "tweets.csv"), quote = TRUE, row.names = FALSE)
saveRDS(tweets, file.path(resultsDir, "tweets.rds"))
```

Exploration

```{r explore}
ggplot(tweets, aes(favorites, retweets)) +
    geom_jitter(size = 3, shape = 21, fill = "#444444", alpha = 0.4) +
    theme_classic(30) + xlab("# favorites") + ylab("# retweets") +
  ggtitle("Score of each tweet of @rbloggers")
# Looks like most posts are close to the (0, 0) area, with 20 favorites and 10 retweets being the maximum boundary for most. A very small fraction of tweets make it past the 40 favorites or 20 retweets. 

tweets %>% arrange(desc(score)) %>% head(10)
# Looks like DataCamp and David Smith have some of the most successful posts. Also note how 9/10 top posts have "R" in their title... Correlation or causation or random? Maybe I should start doing that too then :)
```


```{r by-author}
tweets_by_author <-
  tweets %>% 
  ddply(~ author, function(x) {
    data.frame(num_tweets = nrow(x),
               avg_favorites = mean(x$favorites) %>% round(digits = 1),
               avg_retweets = mean(x$retweets) %>% round(digits = 1),
               avg_score = mean(x$score) %>% round(digits = 1)
    )}) %>%
    arrange(desc(avg_score)) 

tweets_by_author %>% head(10)
# Woo I'm in the top 10! But it looks like the top 10 is dominated by one-hit wonders, so let's try again and only consider blogs that contributed more than one post.

tweets_by_author %>% filter(num_tweets > 1) %>% head(10)
# DataCamp managed to stay up there with over 30 posts, that's impressive. Generally the more posts you have, the harder it is to maintain a high average.

nrow(tweets_by_author)  # 420 unique authors since Sept 2013, so about 1/4 of the authors on r-bloggers haven't posted since

ggplot(tweets_by_author, aes(num_tweets)) +
  geom_histogram(binwidth = 1, fill = "#888888", color = "#444444") +
  theme_classic(30) +
  scale_x_continuous(limits = c(1, 50), breaks = c(1, seq(10, 50, 10))) +
  xlab("# of posts") + ylab("# of blogs who have\nexactly x posts") +
  ggtitle("How much do blogs contribute?") +
  coord_flip()
# looks like a lot of people only posted once since Sept 2013

# who are the top contributors?
tweets_by_author %>% arrange(desc(num_tweets)) %>% head(10)

# I wonder if users who post more also tend to post higher quality content?
cor(tweets_by_author$num_tweets, tweets_by_author$avg_score)

# Now let's see who the best scorers are, which blogs consistently
# contribute posts that get shared a lot
tweets_by_author %>% arrange(desc(avg_score)) %>% head(10)
# First impression: **my name is there!** Woo! :D 
# Looks like a lot of one-hit wonders, which at first seems contradicting to the previous result because it looked like there is no relationship between # of posts and average post score. But this result **does** make sense because of two reasons: first of all, about 1/3 of the authors only have one article, so all other things equal, we expect 1/3 of the top posts to be by them. Secondly, it's much easier to put a lot of effort to produce one great and useful post rather than doing it over and over again.

# Let's see what all the posts looks like for the top scorers (an interactive version of this plot is available at the top of this document)
top_authors <- tweets_by_author %>% arrange(desc(avg_score)) %>% .$author %>% head(10)
tweets_top_authors <- tweets %>% filter(author %in% top_authors)
ggplot(tweets_top_authors) +
    geom_point(aes(favorites, retweets, fill = author), size = 5, shape = 21) +
    theme_classic(30) +
    scale_fill_brewer("Author", type = "qual", palette = 3)

# And let's see them again, in perspective to all tweets
ggplot(tweets) +
  geom_jitter(aes(favorites, retweets), size = 3, shape = 21, fill = "#444444", alpha=0.4) +
  theme_classic(30) +
  geom_point(data = tweets_top_authors,
             aes(favorites, retweets, fill = author), size = 6, shape = 21) +
  scale_fill_brewer("Author", type = "qual", palette = 3)
```

```{r by-day}
tweets_by_day <-
  tweets %>%
  ddply(~ day, function(x) {
    data.frame(num = nrow(x),
               favorites_per_post = mean(x$favorites),
               retweets_per_post = mean(x$retweets),
               avg_score = mean(x$score)
    )})
# cool! looks like weekend (sat/sunday) get the least posts, BUT they both have the highest number of favorites and retweets!

ggplot(tweets, aes(x = day, y = score)) +
    geom_point(aes(fill = day),
               position = position_jitter(width = 0.3),
               show_guide = FALSE, shape = 21, color = "#333333",
               size = 3) +
  geom_line(data = tweets_by_day,
            aes(day, avg_score, group = 1),
            color = "#333333", size = 1) +
  geom_point(data = tweets_by_day,
             aes(day, avg_score),
             size = 9,
             col = "#333333") +
  theme_classic(30) +
  scale_y_continuous(limits=c(-5,100)) +
  scale_x_discrete(labels = levels(tweets_by_day$day) %>% substr(1, 3)) +
  xlab("Day of week") + ylab("Tweet score") +
  ggtitle("Tweet score vs day of tweet")
```


```{r word-cloud}
# wordcloud of most popular words in r-bloggers post titles
topwords <- 
    tweets$title %>%
    paste(collapse = " ") %>%
    str_split("\\s") %>%
    unlist %>%
    tolower %>%
    removePunctuation %>%
    removeWords(stopwords("english")) %>%
    wordStem %>%
    .[. != ""] %>% 
    .[. != "r"] %>%
    table %>%
    sort(decreasing = TRUE) %>%
    head(100)

wordcloud(names(topwords), topwords, min.freq = 10)
```